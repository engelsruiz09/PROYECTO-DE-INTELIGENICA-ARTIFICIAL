{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 1) Importar las librerías estándar de análisis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 2) Importar la clase Naive Bayes\n",
    "import sys\n",
    "sys.path.append(\"..\")  # Para que Python busque un nivel arriba (ajusta según tu ruta)\n",
    "from naivebayes import MultinomialNaiveBayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 3) Cargar datos (ejemplo: Sentiment140)\n",
    "data_path = \"../data/training.1600000.processed.noemoticon.csv\"\n",
    "df = pd.read_csv(data_path, encoding='latin-1', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# df.head() # Explora los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Sentiment140: \n",
    "#   0 => negative, 2 => neutral, 4 => positive\n",
    "# columns: [target, id, date, flag, user, text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df.columns = [\"target\",\"id\",\"date\",\"flag\",\"user\",\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 4) Selecciona columnas relevantes\n",
    "#    Por ejemplo, nos quedamos con 'target' y 'text'\n",
    "df = df[[\"target\",\"text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 5) Para simplificar, mapeamos {0->neg, 2->neu, 4->pos}\n",
    "def map_sentiment(value):\n",
    "    if value == 0:\n",
    "        return \"neg\"\n",
    "    elif value == 2:\n",
    "        return \"neu\"\n",
    "    elif value == 4:\n",
    "        return \"pos\"\n",
    "\n",
    "df[\"sentiment\"] = df[\"target\"].apply(map_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 6) Limpieza / Preprocesamiento\n",
    "#    Aquí puedes hacer todo lo que quieras: quitar URL, stopwords, etc.\n",
    "import re\n",
    "def basic_clean(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+|www\\S+','', text) # quita URLs\n",
    "    text = re.sub(r'@\\w+','', text)          # quita menciones\n",
    "    text = re.sub(r'[^a-záéíóúüñ\\s]', '', text) # deja solo letras\n",
    "    text = re.sub(r'\\s+',' ', text).strip()\n",
    "    return text\n",
    "\n",
    "df[\"clean_text\"] = df[\"text\"].apply(basic_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 7) Tokenización\n",
    "df[\"tokens\"] = df[\"clean_text\"].apply(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 8) Partición Train/Test (puedes usar scikit-learn si lo permiten para splitting,\n",
    "#    o hacerlo manual. Esto NO usa NB, así que no hay problema de usar sklearn\n",
    "#    solo para dividir datos).\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df[\"tokens\"], df[\"sentiment\"], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Convertirlos a listas (por si son pandas Series)\n",
    "X_train = X_train.tolist()\n",
    "X_test = X_test.tolist()\n",
    "y_train = y_train.tolist()\n",
    "y_test = y_test.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 9) Crear y entrenar el modelo\n",
    "nb_model = MultinomialNaiveBayes(alpha=1.0)\n",
    "nb_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 10) Evaluar\n",
    "accuracy = nb_model.score(X_test, y_test)\n",
    "print(f\"Accuracy en test = {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 11) Probar predicción en un tweet nuevo\n",
    "tweet_nuevo = \"I love Python! It's fantastic.\"\n",
    "tweet_clean = basic_clean(tweet_nuevo)\n",
    "tokens_nuevo = tweet_clean.split()\n",
    "\n",
    "prediccion = nb_model.predict([tokens_nuevo])\n",
    "print(\"Tweet nuevo:\", tweet_nuevo)\n",
    "print(\"Predicción:\", prediccion[0])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
